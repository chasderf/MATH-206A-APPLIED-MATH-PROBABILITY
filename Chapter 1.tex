\documentclass{article}
\usepackage{amsmath}
\newcommand\tab[1][1cm]{\hspace*{#1}}
\begin{document}

\title{Math 206A Probability: Chapter 1 continued written by Rick Durrett}
\author{This article is transcribed by Charlie Seager}

\maketitle

\textbf {Chapter 1.3 Random Variables}

\textbf {Theorem 1.3.1} If $\{ \omega : X(\omega) \in A \} \in \mathcal{F}$ for all A $\in \mathcal{A}$ and $\mathcal{A}$ generates S (i.e. S is the smallest $\sigma -$field that contains $\mathcal{A}$) then X is measurable.

\textbf {Theorem 1.3.4} If $X : (\Omega, \mathcal{F}) \rightarrow (S,S)$ and $f : (S,S) \rightarrow (T, \mathcal{T})$ are measurable maps, then f(X) is a measurable map from $\Omega, \mathcal{F}$ to $(T, \mathcal{T})$

\textbf {Theorem 1.3.5} If $X_1 ,..., X_n$ are random variables and $f : (R^n, \mathcal{R}^n) \rightarrow (R, \mathcal{R})$ is measurable, then $f(X_1 ,..., X_n)$ is a random variable.

\textbf {Theorem 1.3.6} If $X_1 ,..., X_n$ are random variables then $X_1 + ... + X_n$ is a random variable.

\textbf {Theorem 1.3.7} If $X_1, X_2,...$ are random variables then so are
\\
\[
\tab \tab {inf X_{n}} \tab {sup X_n} \tab {lim sup X_n} \tab {lim inf X_n} 
\]
\\
\tab \tab {  n} \tab {  n  } \tab {   n   } \tab {    n}

The n is supposed to under each object: substack wasnt cooperating so this is what I produced

\textbf {Chapter 1.4 Integration}

\textbf {Lemma 1.4.1} Let $\varphi$ and $\psi$ be simple functions
\\
(i) If $\varphi \geq 0$ a.e. then $\int \varphi d \mu \geq 0$
\\
(ii) For any $a \in R,$ $\int a\varphi d\mu = a \int \varphi d\mu$ \\
(iii) $\int \varphi + \psi d\mu = \int \varphi d\mu + \int \psi d\mu$

\textbf {Lemma 1.4.2} If (i) and (iii) hold then we have \\
(iv) If $\varphi \leq \psi$ a.e. then $\int \varphi d\mu \leq \int \psi d\mu$ \\
(v) If $\varphi = \psi$ a.e. then $\int \varphi d\mu = \int \psi d\mu$ \\
If in addition, (ii) holds when a = -1 we have \\
(vi) $|\int \varphi d\mu | \leq \int |\varphi| d\mu$

\textbf {Lemma 1.4.3} Let E be a set with $\mu(E) < \infty$. If f and g are bounded function that vanish on $E^c$ then: \\
(i) If $f \geq 0$ a.e. then $\int f d\mu \geq 0$ \\
(ii) For any $a \in R,$  $\int a f d\mu = a \int f d\mu$ \\
(iii) $\int f + g d\mu = \int f d\mu + \int g d\mu$ \\
(iv) If $g \leq f$ a.e. then $\int g d\mu \leq \int f d\mu$ \\
(v) If g = f a.e. then $\int g d\mu = \int f d\mu$ \\
(vi) $| \int f d\mu | \leq \int |f| d\mu$

\textbf {Lemma 1.4.4} Let $E_n \uparrow \Omega$ have $\mu (E_n) < \infty$ and let $a \land b = min(a,b)$. Then 
\begin{center}
$\int_{E_n} f \land n d\mu \uparrow \int f d\mu$ \tab as $n \uparrow \infty$
\end{center}

\textbf {Lemma 1.4.5} Suppose f, $g \geq 0$ \\
(i) $\int f d\mu \geq 0$ \\
(ii) If $a > 0$ then $\int a f d\mu = a \int f d\mu$ \\
(iii) $\int f + g d\mu = \int f d\mu + \int g d\mu$ \\
(iv) If $0 \leq g \leq f$ a.e. then $\int g d\mu \leq \int f d\mu$ \\
(v) If $0 \leq g = f$ a.e. then $\int g d\mu = \int f d\mu$

\textbf {Lemma 1.4.6} If $f = f_1 - f_2$ where $f_1, f_2 \geq 0$ and $\int f_i d\mu < \infty$ then
\begin{center}
$\int f d\mu = \int f_1 d\mu - \int f_2 d\mu$
\end{center}

\textbf {Theorem 1.4.7} Suppose f and g are integrable \\
(i) If $f \geq 0$ a.e. then $\int f d\mu \geq 0$ \\
(ii) For all $a \in R,$  $\int a f d\mu = a \int f d\mu$ \\
(iii) $\int f + g d\mu = \int f d\mu + \int g d\mu$ \\
(iv) If $g \leq f$ a.e. then $\int g d\mu \leq \int g d\mu$ \\
(v) If g = f a.e. then $\int g d\mu = \int f d\mu$
(vi) $| \int f d\mu | \leq \int |f| d\mu$

\textbf {Chapter 1.5 Properties of the Integral}

\textbf {Theorem 1.5.1 Jensen's inequality} Suppose $\varphi$ is convex, that is 
\begin{center}
$\lambda \varphi (x) + (1 - \lambda) \varphi (y) \geq \varphi (\lambda x + (1 - \lambda)y)$
\end{center}
for all $\lambda \in (0,1)$ and $x, y \in R$ If $\mu$ is a probabillity measure and f and $\varphi(f)$ are integrable, then
\begin{center}
$\varphi (\int f d\mu) \leq \int \varphi(f) d\mu$
\end{center}

\textbf {Theorem 1.5.2 Holder's inequality} If $p, q \in (1, \infty)$ with 1/p+1/q=1 then
\begin{center}
$\int | f g| d\mu \leq ||f||_p ||g||_q$
\end{center}

\textbf {Theorem 1.5.3 Bounded convergence theorem} Let E be a set with $\mu(E) < \infty$. Suppose $f_n$ vanishes on $E^c, |f_n (x)| \leq M,$ and $f_n \rightarrow f$ in measure then
\begin{center}
$\int f d\mu =  \lim_{n \to \infty} \int f_n d\mu$
\end{center}

\textbf {Theorem 1.5.5 Fatou's lemma} If $f_n \geq 0$ then 
\begin{center}
$\liminf_{n \to \infty} \int f_n d\mu \geq \int (liminf {f_n}_{n \to \infty}) d\mu$
\end{center}

\textbf {Theorem 1.5.7 Monotone convergence theorem} If $f_n \geq 0$ and $f_n \uparrow f$ then
\begin{center}
$\int f_n d\mu \uparrow \int f d\mu$
\end{center}

\textbf {Theorem 1.5.8 Dominated converges theorem} If $f_n \rightarrow f$ a.e. $|f_n| \leq g$ for all n, and g is integrable, then $\int f_n d\mu \rightarrow \int f d\mu$

\textbf {Chapter 1.6 Expected Value}

\textbf {Theorem 1.6.1} Suppose $X,Y \leq 0$ or $E|X|, E|Y|<\infty$ \\
(a) E(X + Y) = EX + EY \\
(b) E(aX + b) = aE(X) + b for any real numbers a,b \\
(c) If $X \geq Y$ then $EX \geq EY$

\textbf {Chapter 1.6.1 Inequalities: Theorem 1.6.2 Jensen's inequality} Suppose $\varphi$ is convex, that is 
\begin{center}
$\lambda \varphi(x) + (1 - \lambda) \varphi(y) \geq (\lambda x + (1 - \lambda)y)$
\end{center}
for all $\lambda \in (0,1)$ and $x,y \in R$. Then
\begin{center}
$E(\varphi(X)) \geq \varphi(EX)$
\end{center}
provided both expectations exist, i.e. $E|X|$ and $E|\varphi(X)| < \infty$

\textbf {Theorem 1.6.3 Holder's inequality} If $p,q \in [1, \infty]$ with 1/p+1/q - 1 then
\begin{center}
$E|XY| \leq ||X||_p ||Y||_q$
\end{center}
Here $||X||_r = (E|X|^r)^{1/r}$ for $r \in [1, \infty); ||X||_{\infty} = inf \{M: P(|X| > M) = 0\}$

\textbf {Theorem 1.6.4 Chebyshev's inequality} Suppose $\varphi: R \rightarrow R$ has $\varphi \geq 0$, let $A \in \mathcal{R}$ and let $i_A = inf \{ \varphi (y) : y \in A \}$
\begin{center}
$i_A P(X \in A) \leq E(\varphi(X); X \in A) \leq E_{\varphi} (X)$
\end{center}

\textbf {1.6.2 Integration to the Limit: Theorem 1.6.5 Fatou's lemma} If $X_n \geq 0$ then
\begin{center}
$\liminf\limits_{n \to \infty} EX_n \geq E(\liminf\limits_{n \to \infty} X_n)$
\end{center}

\textbf {Theorem 1.6.6 Monotone Convergence theorem} If $0 \leq X_n \uparrow X$ then $EX_n \uparrow EX$.

\textbf {Theorem 1.6.7 Dominated convergence theorem} If $X_n \rightarrow X$ a.s., $|X_n| \leq Y$ for all n, and $EY < \infty$, then $EX_n \rightarrow EX$

\textbf {Theorem 1.6.8} Suppose $X_n \rightarrow X$ a.s. Let g,h be continous functions with
\\ (i) $g \geq 0$ and $g(x) \rightarrow \infty$ as $|x| \rightarrow \infty$ \\
(ii) $|h(x)|/g(x) \rightarrow 0$ as $|x| \rightarrow \infty$ \\
and (iii) $EG(X_n) \leq K < \infty$ for all n\\
Then $Eh(X_n) \rightarrow Eh(X).$ 

\textbf {Chapter 1.6.3 Computing Expected Values: Theorem 1.6.9 Change of variables formula} Let X be a random element of (S,S) with distribution  $\mu, $ i.e. $\mu(A) = P(X \in A)$. If f is a measurable function from (S,S) to (R, $\mathcal{R}$) so that $f \geq 0$ or $E|f(X)| < \infty$, then 
\begin{center}
$Ef(X) = \int_{S} f(y) \mu(dy)$
\end{center}

\textbf {Chapter 1.7 Product Measures, Fubini's Theorem}

\textbf {Theorem 1.7.1} There is a unique measure $\mu$ on $\mathcal{F}$ with
\begin{center}
$\mu(A x B) = \mu_1 (A) \mu_2 (B)$
\end{center}

\textbf {Theorem 1.7.2 Fubini's Theorem} If $f \geq 0$ or $\int |f| d\mu < \infty$ then (*)
\begin{center}
$\int_{X} \int_{Y} f(x,y) \mu_2 (dy) \mu_1 (dx) = \int_{X x Y} f d\mu = \int_{Y} \int_{X} f(x,y) \mu_1(dx) \mu_2(dy)$
\end{center}

\textbf {Lemma 1.7.3} If $E \in \mathcal{F}$ then $E_x \in \mathcal{B}$

\textbf {Lemma 1.7.4} If $E \in \mathcal{F}$ then $g(x) = \mu_2 (E_x)$ is $\mathcal{A}$ measurable and
\begin{center}
$\int_{X} g d\mu_1 = \mu(E)$
\end{center}





\end{document}